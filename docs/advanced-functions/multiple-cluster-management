---
id: manage-multiple-cluster
title: Multiple Cluster Management
sidebar_label: Multiple Cluster Management
---

In a large-scale Kubernetes cluster, you can deploy multiple IOMesh clusters for data isolation, and each IOMesh cluster is an independent storage pool. In this case, the IOMesh CSI driver enables different IOMesh clusters to communicate, reducing the number of CSI drivers that may consume node resources.

![image](https://user-images.githubusercontent.com/102718816/228175494-9d69fac5-de12-4519-a85f-2520c2070f4c.png)

### Deploying Multiple IOMesh Clusters

Note that this release of IOMesh does not support expanding an IOMesh cluster to two or more IOMesh clusters.

**Prerequisites**
- Verify that all requirements in [Prerequisites] are met.
- The IOMesh version should be 1.0.0. 
- A Kubernetes cluster consisting of at least six worker nodes.

**Procedure**

The following section assumes you have 6 worker nodes named `k8s-worker-{0-5}`, deploying the first IOMesh cluster on worker nodes `k8s-worker-0`, `k8s-worker-1`, and `k8s-worker-2` with `namespace` as `iomesh-system` and the second IOMesh cluster on the worker nodes `k8s-worker-3`, `k8s-worker-4`, and `k8s-worker-5` with `iomesh-cluster-1`.

#### Deploying First IOMesh Cluster

1. Set up [`open-iscsi`] on each worker node.

2. Create a YAML config `iomesh.yaml`.

    ```
    helm show values iomesh/iomesh > iomesh-values.yaml
    ```
3. Configure `iomesh.yaml`.

    - Set the field [`iomesh.chunk.dataCIDR`]((../docs/deploy-iomesh-cluster/prerequisites.md#network-requirments)) to the CIDR you configured for the IOMesh storage network.

        ```yaml
        # Source: iomesh-values.yaml
        iomesh:
            chunk:
            dataCIDR: <your-data-cidr-here>
        ```

    - Configure node affinity for fields `iomesh.meta.podPolicy`, `iomesh.chunk.podPolicy`, and `iomesh.redirector.podPolicy` so that they can be scheduled to {0~2} ，对应的字段分别为：

        `iomesh.meta.podPolicy`

        补充 yaml 文件


        `iomesh.chunk.podPolicy`

        ```yaml
        # Source: iomesh-values.yaml
        ...
        iomesh:
        ...
            chunk:
            podPolicy:
                affinity:
                nodeAffinity:
                    requiredDuringSchedulingIgnoredDuringExecution:
                    nodeSelectorTerms:
                    - matchExpressions:
                        - key: kubernetes.io/hostname
                        operator: In
                        values:
                        - k8s-woker-0
                        - k8s-woker-1
                        - k8s-woker-2
        ```

        `iomesh.redirector.podPolicy`

        补充 yaml 文件


    - Configure node affinity for `zookeeper` so that it can be scheduled to `k8s-woker-{0~2}`并增加 pod 反亲和性，在 `iomesh.zookeeper.podPolicy` 中如下配置

        ```yaml
        # Source: iomesh-values.yaml
        ...
        iomesh:
        ...
            zookeeper:
            podPolicy:
                affinity:
                nodeAffinity:
                    requiredDuringSchedulingIgnoredDuringExecution:
                    nodeSelectorTerms:
                    - matchExpressions:
                        - key: kubernetes.io/hostname
                        operator: In
                        values:
                        - k8s-woker-0
                        - k8s-woker-1
                        - k8s-woker-2
                podAntiAffinity:
                    preferredDuringSchedulingIgnoredDuringExecution:
                    - podAffinityTerm:
                        labelSelector:
                        matchExpressions:
                        - key: app
                            operator: In
                            values:
                            - iomesh-zookeeper
                        topologyKey: kubernetes.io/hostname
                    weight: 20
        ```

4. Perform deployment.

   ```shell
   helm install iomesh iomesh/iomesh --create-namespace  --namespace iomesh-system  --values iomesh-values.yaml
   ```

   加一个输出结果

#### Deploying Second IOMesh Cluster
1. Create a namespace for the second IOMesh cluster assumed as `iomesh-cluster-1`.

    ```
    kubectl create ns iomesh-cluster-1
    ```

2. Create the `zookeeper` cluster for the cluster `iomesh-cluster-1`. 

    ```
    kubectl apply -f iomesh-cluster-1-zookeeper.yaml
    ```

3. Set node affinity and pod anti-affinity so that the `zookeeper` cluster can be scheduled to the worker nodes `k8s-woker-3`, `k8s-woker-4`, and `k8s-woker-5`. 

    ```yaml
    # Source: iomesh-cluster-1-zookeeper.yaml
    apiVersion: zookeeper.pravega.io/v1beta1
    kind: ZookeeperCluster
    metadata:
        namespace: iomesh-cluster-1
        name: iomesh-cluster-1-zookeeper
    spec:
        replicas: 3
        image:
        repository: iomesh/zookeeper
        tag: 3.5.9
        pullPolicy: IfNotPresent
        pod:
        affinity:
            nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                - matchExpressions:
                - key: kubernetes.io/hostname
                    operator: In
                    values:
                    - k8s-woker-3
                    - k8s-woker-4
                    - k8s-woker-5
            podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                    matchExpressions:
                    - key: app
                    operator: In
                    values:
                    - iomesh-cluster-1-zookeeper
                topologyKey: kubernetes.io/hostname
                weight: 20
        securityContext:
            runAsUser: 0
        persistence:
        reclaimPolicy: Delete
        spec:
            storageClassName: hostpath
            resources:
            requests:
                storage: 20Gi
    ```

#### Mounting Disks

使用 kubectl edit 编辑每一个 iomesh 对象，参考  https://docs.iomesh.com/deploy/setup-iomesh#block-device-object 配置磁盘

```shell
kubectl edit iomesh -n iomesh-system
kubectl edit iomesh-cluster-1 -n iomesh-cluster-1
```
加输出结果


#### Creating Multiple-Cluster Connection

The IOMesh CSI driver can connect multiple IOMesh clusters, 

一套 IOMesh CSI 可以连接多套 IOMesh 集群。需要创建不同的 ConfigMap 来标识不同的 IOMesh 集群

```yaml
# Source: iomesh-csi-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: iomesh-csi-configmap
  namespace: iomesh-system
data:
  clusterId: k8s-cluster
  iscsiPortal: 127.0.0.1:3260
  metaAddr: iomesh-meta-client.iomesh-system.svc.cluster.local:10100
---
# Source: iomesh-cluster-1-csi-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: iomesh-cluster-1-csi-configmap
  namespace: iomesh-cluster-1
data:
  clusterId: k8s-cluster
  iscsiPortal: 127.0.0.1:3260
  metaAddr: iomesh-cluster-1-meta-client.iomesh-cluster-1.svc.cluster.local:10100
```

```shell
kubectl apply -f iomesh-csi-configmap.yaml
kubectl apply -f iomesh-cluster-1-csi-configmap.yaml
```

ConfigMap 字段释义

| Field | Description |
| ------------- | ----- |
| `data.clusterId` | K8s 集群 ID，用户可自定义，一个 K8s 集群只能有一个，由于两个 iomesh 集群部署在同一 K8s 集群，这个值必须相同 |
| `data.iscsiPortal`   | iscsi 接入点，固定为 127.0.0.1:3260                          |
| `data.metaAddr`      | iomesh meta service 地址，格式为 <iomesh-cluster-name>-meta-client.<iomesh-cluster-namespace>.svc.cluster.local:10100 |

#### Creating StorageClass for Each Cluster

When deploying more than one IOMesh cluster, do not use the default StorageClass `iomesh-csi-driver`; you must create a separate StorageClass for each cluster


```yaml
---
# Source: iomesh-sc.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: iomesh
parameters:
  csi.storage.k8s.io/fstype: ext4
  replicaFactor: "2"
  thinProvision: "true"
  reclaimPolicy: Delete
  clusterConnection: "iomesh-system/iomesh-csi-configmap"  # 此处为上一步创建的主集群 configmap
  iomeshCluster: "iomesh-system/iomesh"
volumeBindingMode: Immediate
provisioner: com.iomesh.csi-driver
allowVolumeExpansion: true
---
# Source: iomesh-cluster-1-sc.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: iomesh-cluster-1
parameters:
  csi.storage.k8s.io/fstype: ext4
  replicaFactor: "2"
  thinProvision: "true"
  reclaimPolicy: Delete
  clusterConnection: "iomesh-cluster-1/iomesh-cluster-1-csi-configmap"  # 此处为上一步创建的边缘集群 configmap
  iomeshCluster: "iomesh-cluster-1/iomesh-cluster-1"
volumeBindingMode: Immediate
provisioner: com.iomesh.csi-driver
allowVolumeExpansion: true
```

StorageClass 的字段释义如下:

| 字段名 | 字段释义 |
| ---------------------------- | ------------------------------------------------------------ |
| parameters.clusterConnection | 在 "创建 CSI 多集群连接配置" 步骤配置的 configmap 的 namespace/name |
| parameters.iomeshCluster     | iomesh 对象的 namespace/name                                 |

其他 StorageClass 字段释义参考 [setup-storageclass](https://docs.iomesh.com/deploy/setup-storageclass)

#### Verifying Deployment 

To verify if the IOMesh clusters are deployed, create a PVC using the StorageClass you created respectively. 

1. Create YAML configs iomesh-pvc.yaml
```yaml
# Source: iomesh-pvc.yaml
kind: PersistentVolumeClaim
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: iomesh-pvc
spec:
  storageClassName: iomesh
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
---
# Source: iomesh-cluster1-pvc.yaml
kind: PersistentVolumeClaim
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: iomesh-cluster1-pvc
spec:
  storageClassName: iomesh-cluster-1
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
```

```shell
kubectl apply -f iomesh-pvc.yaml
kubectl apply -f iomesh-cluster1-pvc.yaml
```

IOMesh 默认会开启拓扑感知功能，当 Pod 使用这些 PVC 时，Pod 会自动调度到对应 IOMesh 集群所在的  K8s Worker 上

### Managing Multiple IOMesh Clusters

#### Upgrade 
在多集群场景下，需要先升级管控 namespace 下的 IOMesh 集群，再升级非管控 namespace 下的 IOMesh 集群

以 [IOMesh 多集群部署](multi-cluster-deploy.md) 中的部署场景为例，升级步骤如下：
1. 参考 [IOMesh 运维](iomesh-operations/cluster-operations.md) 中的升级章节，先升级第一套 IOMesh 集群和 IOMesh 管理组件
2. 第一套 IOMesh 集群升级完毕后，使用 `kubectl edit iomesh iomesh-cluster-1 -n iomesh-cluster-1` 编辑第二套 IOMesh 集群，修改所有的 `*.image.tags`  字段与第一套 IOMesh 集群保持一致

若未按照上述正确的顺序进行升级，比如先升级了第二套集群，再升级第一套，则在升级第二套集群时该集群可能会暂时处于一个错误状态。当第一套被升级完成后，两个集群都会最终达到正确状态

#### Scaling
IOMesh 实例扩容方式与 [IOMesh 运维](iomesh-operations/cluster-operations.md) 中扩容章节保持一致

#### Uninstalling 

在多集群场景下，需要先卸载非管控 namespace 下的 IOMesh 集群，再卸载管控 namespace 下的 IOMesh 集群 IOMesh 集群

以 [IOMesh 多集群部署](multi-cluster-deploy.md) 中的场景为例，卸载步骤如下：
1. 卸载第二套 IOMesh 集群, 同时删除集群中的 iomesh 和 zookeeper

```shell
kubectl delete -f iomesh-cluster-1-zookeeper.yaml && kubectl delete -f iomesh-cluster-1.yaml
```

2. 参考 [IOMesh 运维](iomesh-operations/cluster-operations.md) 中的卸载章节，卸载第一套 IOMesh 集群和 IOMesh 管理组件

若未按照上述正确的顺序进行卸载，比如先卸载了第一套集群，再卸载第二套，可能会造成部分资源在该 namespace 下残留需要手动处理，如果不清理可能影响下一次在这个 namespace 下部署

#### License Management
多个 IOMesh 集群间的 license 相互独立，每个 IOMesh 集群拥有一个独立的集群序列号，当需要激活 license 时请参考 https://www.iomesh.com/license，每个 IOMesh 集群需要被单独激活