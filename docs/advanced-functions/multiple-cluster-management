---
id: manage-multiple-cluster
title: Multiple Cluster Management
sidebar_label: Multiple Cluster Management
---

In a large-scale Kubernetes cluster, you can deploy multiple IOMesh clusters for data isolation, and each IOMesh cluster is an independent storage pool. In this case, the IOMesh CSI driver enables different IOMesh clusters to communicate, reducing the number of CSI drivers that may consume node resources.

![image](https://user-images.githubusercontent.com/102718816/228175494-9d69fac5-de12-4519-a85f-2520c2070f4c.png)

### Multiple Cluster Deployment

To reduce the number of Pods required in a multi-cluster deployment of IOMesh, the management components shared by all IOMesh clusters, including the IOMesh Operator, IOMesh CSI driver, and Node Disk Manager, will be installed on the first IOMesh cluster, which is referred to as the management cluster.

> _Note:_
> Expanding an IOMesh cluster to two or more clusters is not supported. You should 

**Prerequisites**
- Verify that all requirements in [Prerequisites](https://docs.iomesh.com/deploy-iomesh-cluster/prerequsites.md) are met.
- The IOMesh version should be 1.0.0. 
- A Kubernetes cluster consisting of at least six worker nodes.

**Procedure**

The following section assumes you have 6 worker nodes, deploying the first cluster `iomesh-cluster` on worker nodes `k8s-worker-0`, `k8s-worker-1`, and `k8s-worker-2` and the second cluster `iomesh-cluster-1` on worker nodes `k8s-worker-3`, `k8s-worker-4`, and `k8s-worker-5`. 

#### Deploying First IOMesh Cluster

1. Set up [`open-iscsi`](../docs/deploy-iomesh-cluster/setup-worker-node.md) on each worker node.

2. Export the YAML config `iomesh.yaml`. 

    ```
    helm show values iomesh/iomesh > iomesh.yaml
    ```
3. Configure `iomesh.yaml`.

    - Set the field [`iomesh.chunk.dataCIDR`]((../docs/deploy-iomesh-cluster/prerequisites.md#network-requirments)) to the CIDR you configured for the IOMesh storage network.

        ```yaml
        iomesh:
            chunk:
            dataCIDR: <your-data-cidr-here>
        ```

    - Configure node affinity for fields `iomesh.meta.podPolicy`, `iomesh.chunk.podPolicy`, and `iomesh.redirector.podPolicy` so that they can be scheduled to `k8s-worker-0`, `k8s-worker-1`, and `k8s-worker-2`.


        `iomesh.meta.podPolicy`

        补充 yaml 文件 和 key/value 如何配置


        `iomesh.chunk.podPolicy`

        ```yaml
        ...
        iomesh:
        ...
            chunk:
            podPolicy:
                affinity:
                nodeAffinity:
                    requiredDuringSchedulingIgnoredDuringExecution:
                    nodeSelectorTerms:
                    - matchExpressions:
                        - key: kubernetes.io/hostname
                        operator: In
                        values:
                        - k8s-woker-0
                        - k8s-woker-1
                        - k8s-woker-2
        ```
      补充 key/value 如何配置

        `iomesh.redirector.podPolicy`

        补充 yaml 文件 和 key/value 如何配置


    - Configure `nodeAffinity` to schedule `zookeeper` to `k8s-worker-0`, `k8s-worker-1`, and `k8s-worker-2` and `podAntiAffinity` to ensure each `zookeeper` pod resides on a node, avoiding a single point of failure.  
    
      - Locate `nodeAffinity` and `podAntiAffinity`, you will see the content below:
      ```yaml
      ...
        iomesh:
        ...
            zookeeper:
            podPolicy:
                affinity:
                nodeAffinity:
                podAntiAffinity:
      ```
    
      - Copy and paste the following sample code and configure `key` and `values`.

        ```yaml
        ...
        iomesh:
        ...
            zookeeper:
            podPolicy:
                affinity:
                nodeAffinity:
                    requiredDuringSchedulingIgnoredDuringExecution:
                    nodeSelectorTerms:
                    - matchExpressions:
                        - key: kubernetes.io/hostname # 配置 key/values
                        operator: In
                        values:
                        - k8s-woker-0
                        - k8s-woker-1
                        - k8s-woker-2
                podAntiAffinity:
                    preferredDuringSchedulingIgnoredDuringExecution:
                    - podAffinityTerm:
                        labelSelector:
                        matchExpressions:
                        - key: app
                            operator: In
                            values:
                            - iomesh-zookeeper 
                        topologyKey: kubernetes.io/hostname
                    weight: 20 
        ```

4. Perform deployment.

   ```shell
   helm install iomesh iomesh/iomesh --create-namespace  --namespace iomesh-system  --values iomesh.yaml
   
   ```
  If deployment is successful, you should see an output like:
   ```output
   NAME                                                  READY   STATUS    RESTARTS   AGE
   iomesh-blockdevice-monitor-766655959f-7bwvv           1/1     Running   0          5h37m
   iomesh-blockdevice-monitor-prober-pgbcp               1/1     Running   0          5h36m
   iomesh-blockdevice-monitor-prober-v848k               1/1     Running   0          5h37m
   iomesh-blockdevice-monitor-prober-w6vrw               1/1     Running   0          5h36m
   iomesh-chunk-0                                        3/3     Running   0          5h34m
   iomesh-chunk-1                                        3/3     Running   0          5h33m
   iomesh-chunk-2                                        3/3     Running   0          5h33m
   iomesh-csi-driver-controller-plugin-ccf6ccfd9-kf296   6/6     Running   3          30h
   iomesh-csi-driver-node-plugin-c8mrj                   3/3     Running   1          30h
   iomesh-csi-driver-node-plugin-jncfr                   3/3     Running   1          30h
   iomesh-csi-driver-node-plugin-qrvc6                   3/3     Running   1          30h
   iomesh-hostpath-provisioner-vdmrz                     1/1     Running   0          31h
   iomesh-hostpath-provisioner-wtbfh                     1/1     Running   0          31h
   iomesh-hostpath-provisioner-xlkvb                     1/1     Running   0          31h
   iomesh-iscsi-redirector-9c455                         2/2     Running   1          5h36m
   iomesh-iscsi-redirector-k7jbx                         2/2     Running   3          5h35m
   iomesh-iscsi-redirector-l9w7l                         2/2     Running   1          5h34m
   iomesh-localpv-manager-82564                          4/4     Running   2          29h
   iomesh-localpv-manager-pkcbd                          4/4     Running   0          12h
   iomesh-localpv-manager-px4cl                          4/4     Running   0          29h
   iomesh-meta-0                                         2/2     Running   0          5h34m
   iomesh-meta-1                                         2/2     Running   0          5h35m
   iomesh-meta-2                                         2/2     Running   0          5h34m
   iomesh-openebs-ndm-cluster-exporter-68c757948-qmtt4   1/1     Running   0          31h
   iomesh-openebs-ndm-gnkzt                              1/1     Running   0          31h
   iomesh-openebs-ndm-lrww8                              1/1     Running   0          31h
   iomesh-openebs-ndm-node-exporter-rp4qr                1/1     Running   0          31h
   iomesh-openebs-ndm-node-exporter-sgzpp                1/1     Running   0          31h
   iomesh-openebs-ndm-node-exporter-txbn5                1/1     Running   0          31h
   iomesh-openebs-ndm-operator-584fdbcb94-2tjsp          1/1     Running   0          31h
   iomesh-openebs-ndm-zdngg                              1/1     Running   0          31h
   iomesh-zookeeper-0                                    1/1     Running   0          31h
   iomesh-zookeeper-operator-64957fcc4f-zpdjp            1/1     Running   0          30h
   operator-f5644b7f9-2vvw7                              1/1     Running   0          5h37m
   ```

#### Deploying Second IOMesh Cluster (只需要 Zk 和 ZBS)
1. Create a namespace for the second IOMesh cluster `iomesh-cluster-1`.

    ```
    kubectl create namespace iomesh-cluster-1
    ```

2. Create the `zookeeper` cluster for the cluster `iomesh-cluster-1`. 

    ```
    kubectl apply -f iomesh-cluster-1-zookeeper.yaml
    ```

3. Create a YAML config iomesh-cluster-1-zookeeper.yaml and configure node affinity and pod anti-affinity so that the `zookeeper` cluster can be scheduled to the worker nodes `k8s-woker-3`, `k8s-woker-4`, and `k8s-woker-5`. 


    ```yaml
    # Source: iomesh-cluster-1-zookeeper.yaml
    apiVersion: zookeeper.pravega.io/v1beta1
    kind: ZookeeperCluster
    metadata:
        namespace: iomesh-cluster-1
        name: iomesh-cluster-1-zookeeper
    spec:
        replicas: 3 
        image:
        repository: iomesh/zookeeper
        tag: 3.5.9
        pullPolicy: IfNotPresent
        pod:
        affinity:
            nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                - matchExpressions:
                - key: kubernetes.io/hostname # 补充如何配置 key.values
                    operator: In
                    values:
                    - k8s-woker-3
                    - k8s-woker-4
                    - k8s-woker-5
            podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                    matchExpressions:
                    - key: app
                    operator: In
                    values:
                    - iomesh-cluster-1-zookeeper
                topologyKey: kubernetes.io/hostname
                weight: 20
        securityContext:
            runAsUser: 0
        persistence:
        reclaimPolicy: Delete
        spec:
            storageClassName: hostpath
            resources:
            requests:
                storage: 20Gi
    ```

#### Mounting Disks

1. 查看设备是在 iomesh-system namespace 
kubectl --namespace iomesh-system -o wide get blockdevice


https://docs.iomesh.com/deploy/setup-iomesh#block-device-object 

2. 使用 kubectl edit 编辑每一个 iomesh 对象，参考  https://docs.iomesh.com/deploy/setup-iomesh#block-device-object 配置磁盘

```shell
kubectl edit iomesh -n iomesh-system
kubectl edit iomesh-cluster-1 -n iomesh-cluster-1
```

#### Configuring Multiple-Cluster Connection

The IOMesh CSI driver can connect multiple IOMesh clusters, configmap 包含了连接信息

一套 IOMesh CSI 可以连接多套 IOMesh 集群。需要创建不同的 ConfigMap 来标识不同的 IOMesh 集群

```yaml
# Source: iomesh-csi-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: iomesh-csi-configmap
  namespace: iomesh-system
data:
  clusterId: k8s-cluster
  iscsiPortal: 127.0.0.1:3260
  metaAddr: iomesh-meta-client.iomesh-system.svc.cluster.local:10100
---



# Source: iomesh-cluster-1-csi-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: iomesh-cluster-1-csi-configmap
  namespace: iomesh-cluster-1
data:
  clusterId: k8s-cluster
  iscsiPortal: 127.0.0.1:3260
  metaAddr: iomesh-cluster-1-meta-client.iomesh-cluster-1.svc.cluster.local:10100
```

```shell
kubectl apply -f iomesh-csi-configmap.yaml
kubectl apply -f iomesh-cluster-1-csi-configmap.yaml
```

ConfigMap

| Field | Description |
| ----- | ----- |
| `data.clusterId` | The Kubernetes cluster ID, which is customizable. Since a Kubernetes cluster can only have one cluster ID, two iomesh clusters deployed in the same K8s cluster must have the same field ID filled in.|
| `data.iscsiPortal`| iscsi access point, fixed to 127.0.0.1:3260.|
| `data.metaAddr`      | iomesh meta service address, which follows the format: <iomesh-cluster-name>-meta-client.<iomesh-cluster-namespace>.svc.cluster.local:10100. |

如果没有报错，就 apply 成功。

#### Creating StorageClass for Each Cluster

When deploying more than one IOMesh cluster, do not use the default StorageClass `iomesh-csi-driver`; you must [create a separate StorageClass](https://docs.iomesh.com/deploy-iomesh-cluster/setup-iomesh) for each cluster.

|Field| Description|
|---|---|
| `parameters.clusterConnection` | 在 "创建 CSI 多集群连接配置" 步骤配置的 configmap 的 namespace/name |
| `parameters.iomeshCluster`| The namespace where the IOMesh cluster is located/the IOMesh cluster name.                              |

**Procedure**

1. Create 
```yaml
---
# Source: iomesh-sc.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: iomesh
parameters:
  csi.storage.k8s.io/fstype: "ex4" 
  replicaFactor: "2" # 
  thinProvision: "true"
  reclaimPolicy: Delete
  clusterConnection: "iomesh-system/iomesh-csi-configmap"  # 此处为上一步创建的主集群 configmap
  iomeshCluster: "iomesh-system/iomesh"
volumeBindingMode: Immediate
provisioner: com.iomesh.csi-driver
allowVolumeExpansion: true
---

# Source: iomesh-cluster-1-sc.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: iomesh-cluster-1
parameters:
  csi.storage.k8s.io/fstype: ext4
  replicaFactor: "2"
  thinProvision: "true"
  reclaimPolicy: Delete
  clusterConnection: "iomesh-cluster-1/iomesh-cluster-1-csi-configmap"  # 此处为上一步创建的边缘集群 configmap (什么是边缘集群)
  iomeshCluster: "iomesh-cluster-1/iomesh-cluster-1"
volumeBindingMode: Immediate
provisioner: com.iomesh.csi-driver
allowVolumeExpansion: true
```

#### Verifying Deployment 

To verify if the IOMesh clusters are deployed, create a PVC using the StorageClass you created respectively. 

1. Create the YAML config `iomesh-pvc.yaml`. Apply the YAML config to create the PVC for the first IOMesh cluster.

    ```yaml
    kind: PersistentVolumeClaim
    apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      name: iomesh-pvc
    spec:
      storageClassName: iomesh
      accessModes:
        - ReadWriteOnce
      resources:
        requests:
          storage: 1Gi
    ```

    ```shell
    kubectl apply -f iomesh-pvc.yaml
    ```
    链到前面的结果

2. Create the second YAML config `iomesh-cluster1-pvc.yaml`. Then apply the YAML config to create the PVC for the second IOMesh cluster.

    ```yaml
    kind: PersistentVolumeClaim
    apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      name: iomesh-cluster1-pvc
    spec:
      storageClassName: iomesh-cluster-1
      accessModes:
        - ReadWriteOnce
      resources:
        requests:
          storage: 1Gi
    ```

    ```shell
    kubectl apply -f iomesh-cluster1-pvc.yaml
    ```

    链到前面的 example

    IOMesh 默认会开启拓扑感知功能，当 Pod 使用这些 PVC 时，Pod 会自动调度到对应 IOMesh 集群所在的  K8s Worker 上.（这句话读不通）

### Multiple Cluster Operations

#### Upgrading Multiple Clusters

When upgrading multiple IOMesh clusters, it is recommended to first upgrade the management cluster and then other clusters. If you choose a cluster not the management cluster, the cluster will be temporarily unavailable when upgrading the second cluster; however all clusters will be back to normal finally.

**Procedure**

1. Upgrade the first IOMesh cluster, which is the management cluster by referring to [Upgrading Cluster](iomesh-operations/cluster-operations.md).

2. 第一套 IOMesh 集群升级完毕后，使用 `kubectl edit iomesh iomesh-cluster-1 -n iomesh-cluster-1` 编辑第二套 IOMesh 集群，修改所有的 `*.image.tags`  字段与第一套 IOMesh 集群保持一致。（这一段 image.tags 是啥，都要补充）

#### Scaling Multiple Clusters
IOMesh 实例（什么是实例）扩容方式与 [IOMesh 运维](iomesh-operations/cluster-operations.md) 中扩容章节保持一致. 一个一个来

#### Uninstalling Multiple Clusters

When uninstalling more than one IOMesh cluster, uninstall the other clusters first and then the management cluster last.

以 [IOMesh 多集群部署](multi-cluster-deploy.md) 中的场景为例，卸载步骤如下：

**Procedure**

1. Uninstall the second IOMesh cluster and delete `iomesh


卸载第二套 IOMesh 集群, 同时删除集群中的 iomesh 和 zookeeper

    ```shell
    kubectl delete -f iomesh-cluster-1-zookeeper.yaml && kubectl delete -f iomesh-cluster-1.yaml
    ```

2. 参考 [IOMesh 运维](iomesh-operations/cluster-operations.md) 中的卸载章节，卸载第一套 IOMesh 集群和 IOMesh 管理组件

若未按照上述正确的顺序进行卸载，比如先卸载了第一套集群，再卸载第二套，可能会造成部分资源在该 namespace 下残留需要手动处理(怎么处理），如果不清理可能影响下一次在这个 namespace 下部署

#### License Management

Each IOMesh cluster has a license with a unique serial number. You need to [activate the license](https://www.iomesh.com/license) for each IOMesh cluster respectively.